{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lighthouse Labs\n",
    "### W08D2 Deep Learning Architectures\n",
    "Instructor: Socorro Dominguez  \n",
    "November 03, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Agenda:**\n",
    "\n",
    "* Different DL Architectures\n",
    "    * CNN\n",
    "        * Image Recognition\n",
    "    * RNN\n",
    "        * LSTM\n",
    "    \n",
    "* Best practices when designing a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Different DL Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convolutional Neural Networks (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "CNNs are widely used in computer vision problems. Their input is usually images and their output is usually probabilities of being something. \n",
    "\n",
    "A CNN is a special kind of FeedForward NN that reduces the number of parameters in a deep neural network with many units without losing too much in the quality of the model. \n",
    "\n",
    "In images, pixels that are close to one another usually have the same type of information: sky, water, leaves, etc. \n",
    "\n",
    "The exception from the rule are **the edges**: the parts of an image where two different objects “touch” one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We train the NN to recognize regions of the same information as well as the edges. This would allow the NN to predict the object represented in the image. \n",
    "\n",
    "For example, if the neural network detected multiple skin regions and edges that look like parts of an oval with skin-like tone on the inside and bluish tone on the outside, then it is likely that it’s a face on a sky background. \n",
    "\n",
    "If our goal is to detect people on pictures, the neural network will most likely succeed in predicting a person in this picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The most important information in the image is local\n",
    "\n",
    "We can split the image into square patches using a moving window approach. We can train multiple smaller regression models at once, each small regression model receiving a square patch as input. The goal of each small regression model is to learn to detect a specific kind of pattern in the input patch. \n",
    "\n",
    "For example, one small regression model will learn to detect the sky; another one will detect the grass, the third one will detect edges of a building, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In CNNs, a small regression model looks like:\n",
    "\n",
    "![img](img/01_CNN.png)\n",
    "\n",
    "\n",
    "This only has the layer 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To detect some pattern, a small regression model has to learn the parameters of a matrix F (for “filter”) of size p × p, where p is the size of a patch.\n",
    "\n",
    "\n",
    "Let’s assume that the input image is black and white, with 1 representing black and 0 representing white pixels. Assume also that our patches are 3 by 3 pixels (p = 3). Some patch could then look like the following matrix P (for “patch”):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$P = \\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & 1 & 1 \\\\ 0 & 1 & 0 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The previous patch represents a pattern that looks like a cross. \n",
    "\n",
    "The small regression model that will detect such patterns (and only them) would need to learn a 3 by 3 parameter matrix F where parameters at positions corresponding to the 1s in the input patch would be positive numbers, while the parameters in positions corresponding to 0s would be close to zero. \n",
    "\n",
    "If we calculate the convolution of matrices P and F, the value we obtain is higher the more similar F is to P. To illustrate the convolution of two matrices, assume that F looks like this:\n",
    "\n",
    "$$F = \\begin{bmatrix} 0 & 2 & 3 \\\\ 2 & 4 & 1 \\\\ 0 & 3 & 0 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then convolution operator is only defined for matrices that have the same number of rows and columns. For our matrices of P and F it’s calculated as illustrated below:\n",
    "\n",
    "![convolution](img/02_Convolution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If our patch had a different pattern, then the convolution with F would give a different result. \n",
    "\n",
    "*The more the patch “looks” like the filter, the higher the value of the convolution operation is*\n",
    "\n",
    "For convenience, there’s also a bias parameter b associated with each filter F which is added to the result of a\n",
    "convolution before applying the nonlinearity (activation function)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One layer of a CNN consists of multiple convolution filters (each with its own bias parameter).\n",
    "\n",
    "Each filter of the first (leftmost) layer slides — or convolves — across the input image, left to right, top to bottom, and convolution is computed at each iteration.\n",
    "\n",
    "Like this:\n",
    "\n",
    "![convolv](img/03_Convolving.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If the CNN has one convolution layer following another convolution layer, then the subsequent layer *l + 1* treats the output of the preceding layer *l* as a collection of size *l* image matrices.\n",
    "\n",
    "Hyperparameters:\n",
    "* **Stride**\n",
    "* **Padding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Pooling**\n",
    "\n",
    "This is a technique very often used in CNNs. Pooling works in a way very similar to convolution, as a filter applied using amoving window approach. \n",
    "\n",
    "Instead of applying a trainable filter to an input matrix, a pooling layer applies a fixed operator, usually either max or average. \n",
    "\n",
    "Pooling's hyperparameters are also the size of the filter and the stride. \n",
    "\n",
    "Usually, a pooling layer follows a convolution layer, and it gets the output of convolution as input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Poolingdoesn’t have parameters to learn. It also contributes to the increased accuracy of the model and improves the speed of training by reducing the number of parameters of the neural network.\n",
    "![pooling](img/04_Pooling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ReLU as Normalization Technique\n",
    "\n",
    "After getting our new convolved matrix, turn anything negative to zero.\n",
    "\n",
    "This removes unnecessary noise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What a CNN looks like after all?\n",
    "\n",
    "![img](img/05_FullCNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Implementing Example in Keras**   \n",
    "\n",
    "We'll move on to the famous the MNIST digits -- a classic dataset for deep learning. The MNIST data set is  bigger than the digits dataset built into sklearn: the images are larger ($28\\times28$ instead of $8\\times8$) and there are more of them ($70000$ insetad of $1797$). In total, we're dealing with $70000\\times28\\times28\\approx 55$ million training pixels instead of $1797\\times8\\times8\\approx80000$ training pixels (about $500$ times more data). \n",
    "\n",
    "The following code loads the MNIST dataset. The first time you run it, the data will be downloaded. In future times, it will use the local version.\n",
    "\n",
    "*Close presentation view*\n",
    "\n",
    "\n",
    "Also check FMNIST Fashion Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "# keras imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.datasets\n",
    "\n",
    "# other imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, shuffled and split between train and test sets\n",
    "(X_train_img, y_train), (X_test_img, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display a random training example\n",
    "i = np.random.randint(0,len(X_train_img))\n",
    "plt.imshow(X_train_img[i],cmap='gray')\n",
    "plt.title('This is a %d' % y_train[i]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_img.reshape(60000, 28*28)\n",
    "X_test = X_test_img.reshape(10000, 28*28)\n",
    "X_train = X_train / 255 # this is the same a scaling, since the pixel intensities range from 0 to 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2600 - accuracy: 0.9246 - val_loss: 0.1235 - val_accuracy: 0.9633\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0966 - accuracy: 0.9709 - val_loss: 0.0898 - val_accuracy: 0.9727\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0645 - accuracy: 0.9802 - val_loss: 0.0737 - val_accuracy: 0.9763\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0457 - accuracy: 0.9852 - val_loss: 0.0714 - val_accuracy: 0.9790\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0323 - accuracy: 0.9901 - val_loss: 0.0927 - val_accuracy: 0.9717\n",
      "Train accuracy: 0.987416684627533\n",
      "Test accuracy: 0.9717000126838684\n",
      "---Running Time: 10.694352149963379 seconds ---\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 5\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, epochs=epochs,\n",
    "                    verbose=1, validation_data=(X_test, Y_test))\n",
    "\n",
    "score = model.evaluate(X_train, Y_train, verbose=0)\n",
    "print('Train accuracy:', score[1])\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "elapsed_time = time.time()-start_time\n",
    "print(\"---Running Time: %s seconds ---\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "RNNs are used to label, classify, or generate sequences. \n",
    "\n",
    "A sequence is a matrix, each row of which is a feature vector and the order of rows matters.\n",
    "\n",
    "To label a sequence is to predict a class for each feature vector in a sequence. To classify a sequence is to predict a class for the entire sequence. To generate a sequence is to output another sequence (of a possibly different length) somehow relevant to the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "RNNs are often used in text processing because sentences and texts are naturally sequences of either words and punctuation marks or sequences of characters. RNN's are also used in speech processing.\n",
    "\n",
    "RNNs are not feed-forward: they contains loops. The idea is that each unit *u* of recurrent layer *l* has a real-valued state hl,u. \n",
    "\n",
    "The state can be seen as the memory of the unit. In RNN, each unit *u* in each layer *l* receives two inputs: a vector of states from the previous layer l − 1 and the vector of states from this same layer *l* from the previous time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let’s consider the first and the second recurrent layers of an RNN. The first (leftmost) layer receives a feature vector as input. The second layer receives the output of the first layer as input.\n",
    "\n",
    "![RNN](img/04_RNN.png)\n",
    "\n",
    "\n",
    "Each training example is a matrix in which each row is a feature vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In an RNN, the feature vectors from an input example are “read” by the NN sequentially in the order of the timesteps. \n",
    "\n",
    "\n",
    "The weights are computed from the training data using gradient descent with backpropagation. To train RNN models, a special version of backpropagation is used called backpropagation through time.\n",
    "\n",
    "**Disadvantages**\n",
    "* Most of the times, tanh and softmax are used but they both suffer from the vanishing gradient problem. \n",
    "\n",
    "* Even with only one or two recurrent layers, the longer is the input sequence, the deeper is the unfolded network.\n",
    "\n",
    "* Long-term dependencies. As the length of the input sequence grows, the feature vectors from the beginning of the sequence tend to be “forgotten,” In text or speech processing, the cause-effect link between distant words in a long sentence can be lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The most effective RNN models used in practice are gated RNNs. These include the long short-term memory (**LSTM**) networks, which can store information in their units for future use, much like bits in a computer’s memory. \n",
    "\n",
    "The difference with the real memory is that reading, writing, and erasure of information stored in each unit is controlled by activation functions. \n",
    "\n",
    "The trained NN can “read” the input sequence of feature vectors and decide at some early time step t to keep specific information about the feature vectors. That information about the earlier feature vectors can later be used by the model to process the feature vectors from near the end of the input sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example:** If the input text starts with the word *she*, a language processing RNN model could decide to store the information about the gender to interpret correctly the word their seen later in the sentence.\n",
    "\n",
    "Units make decisions about what information to store, and when to allow reads, writes, and erasures. Those decisions are learned from data and implemented through the concept of gates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Other important extensions to RNNs include bi-directional RNNs, RNNs with attention and sequence-to-sequence RNN models. The latter, in particular, are frequently used to build neural machine translation models and other models for text to text transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What do RNN's look like**\n",
    "![rnn](img/06_RNN_viz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![LSTM](img/07_LSTM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Building RNN's**\n",
    "\n",
    "* Out of presentation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, re\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\" One , two , three , four , five \n",
    "Once I caught a fish alive . \n",
    "Six , seven , eight , nine , ten \n",
    "Then I let it go again . \n",
    "Why did you let it go ? \n",
    "Because it bit my finger so . \n",
    "Which finger did it bite ? \n",
    "This little finger on my right . \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'it': 1,\n",
       " 'finger': 2,\n",
       " 'i': 3,\n",
       " 'let': 4,\n",
       " 'go': 5,\n",
       " 'did': 6,\n",
       " 'my': 7,\n",
       " 'one': 8,\n",
       " 'two': 9,\n",
       " 'three': 10,\n",
       " 'four': 11,\n",
       " 'five': 12,\n",
       " 'once': 13,\n",
       " 'caught': 14,\n",
       " 'a': 15,\n",
       " 'fish': 16,\n",
       " 'alive': 17,\n",
       " 'six': 18,\n",
       " 'seven': 19,\n",
       " 'eight': 20,\n",
       " 'nine': 21,\n",
       " 'ten': 22,\n",
       " 'then': 23,\n",
       " 'again': 24,\n",
       " 'why': 25,\n",
       " 'you': 26,\n",
       " 'because': 27,\n",
       " 'bit': 28,\n",
       " 'so': 29,\n",
       " 'which': 30,\n",
       " 'bite': 31,\n",
       " 'this': 32,\n",
       " 'little': 33,\n",
       " 'on': 34,\n",
       " 'right': 35}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([corpus])\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus:   One , two , three , four , five \n",
      "Once I caught a fish alive . \n",
      "Six , seven , eight , nine , ten \n",
      "Then I let it go again . \n",
      "Why did you let it go ? \n",
      "Because it bit my finger so . \n",
      "Which finger did it bite ? \n",
      "This little finger on my right . \n",
      "\n",
      "Tokenizer word index:  {'it': 1, 'finger': 2, 'i': 3, 'let': 4, 'go': 5, 'did': 6, 'my': 7, 'one': 8, 'two': 9, 'three': 10, 'four': 11, 'five': 12, 'once': 13, 'caught': 14, 'a': 15, 'fish': 16, 'alive': 17, 'six': 18, 'seven': 19, 'eight': 20, 'nine': 21, 'ten': 22, 'then': 23, 'again': 24, 'why': 25, 'you': 26, 'because': 27, 'bit': 28, 'so': 29, 'which': 30, 'bite': 31, 'this': 32, 'little': 33, 'on': 34, 'right': 35}\n",
      "\n",
      "Encoded corpus:  [8, 9, 10, 11, 12, 13, 3, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 3, 4, 1, 5, 24, 25, 6, 26, 4, 1, 5, 27, 1, 28, 7, 2, 29, 30, 2, 6, 1, 31, 32, 33, 2, 34, 7, 35]\n"
     ]
    }
   ],
   "source": [
    "print('Corpus: ', corpus)\n",
    "encoded = tokenizer.texts_to_sequences([corpus])[0]\n",
    "print('\\nTokenizer word index: ', tokenizer.word_index)\n",
    "print('\\nEncoded corpus: ', encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  [ 8  9 10 11 12 13  3 14 15 16 17 18 19 20 21 22 23  3  4  1  5 24 25  6\n",
      " 26  4  1  5 27  1 28  7  2 29 30  2  6  1 31 32 33  2 34  7]\n",
      "\n",
      "y:  [ 9 10 11 12 13  3 14 15 16 17 18 19 20 21 22 23  3  4  1  5 24 25  6 26\n",
      "  4  1  5 27  1 28  7  2 29 30  2  6  1 31 32 33  2 34  7 35]\n"
     ]
    }
   ],
   "source": [
    "# Why add one? \n",
    "# To make it work with the embedding layer.\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "sequences = []\n",
    "\n",
    "for i in range(1,len(encoded)):\n",
    "    sequences.append(encoded[i-1:i+1])\n",
    "sequences = array(sequences)\n",
    "\n",
    "## Create X and y\n",
    "X, y = sequences[:,0],sequences[:,1]\n",
    "print('X: ', X)\n",
    "print('\\ny: ', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1, 10)             360       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                12200     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 36)                1836      \n",
      "=================================================================\n",
      "Total params: 14,396\n",
      "Trainable params: 14,396\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Build and compile network \n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length = 1))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fae1ce1b910>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(X, y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 'fish'\n",
    "def print_next_word(seed):\n",
    "  encoded = tokenizer.texts_to_sequences([seed])[0]\n",
    "  encoded = array(encoded)\n",
    "  yhat = model.predict_classes(encoded, verbose=0)\n",
    "\n",
    "  for word, index in tokenizer.word_index.items():\n",
    "    if index == yhat:\n",
    "      print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out Andre Karpathy's blog: [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
