{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lighthouse Labs\n",
    "### W05D04 Naive Bayes\n",
    "Instructor: Socorro Dominguez  \n",
    "October 15, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Second Half Agenda:**\n",
    "* Conditional Probability Review\n",
    "    * Bayes theorem\n",
    "    \n",
    "    \n",
    "* Naive Bayes\n",
    "    * Multionomial\n",
    "    * Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review: Conditional probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Brain teaser 1**: \n",
    "\n",
    "1. A couple has two children. The older child is a girl. \n",
    "2. A couple has two children. One of them is a girl. \n",
    "\n",
    "In each case, what's the probability that the other child is also a girl? (The two cases are not the same!)  \n",
    "Warning: This exercise assumes `Gender` as a binary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definitions\n",
    "\n",
    "- Conditional probabilities are a way of using information we have about random variables.\n",
    "- For example, for a fair 6-sided dice, what if we already know the roll is odd, because someone told us. What are the 6 _conditional_ probabilities of each outcome?\n",
    "  - They are: ...\n",
    "- We write conditional probabilities with a vertical bar, `|`. The information we're conditioning on goes after the bar.\n",
    "  - E.g., $P(X=i \\mid \\text{X is odd})$ is a **conditional probability**\n",
    "  - The set of these values form the **conditional distribution**\n",
    "    - Conditional distributions are still probability distributions - they must sum up to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- So, what's the pattern here?\n",
    "  - The conditioning _eliminates some possible outcomes_.\n",
    "  - For the remaining outcomes, we _renormalized_ the distribution.\n",
    "  - That is, we took the proportion of the allowed outcomes that satisfy the event description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Brain teaser Solution:**\n",
    "\n",
    "The 4 equally likely possibilities are (with the first one being older): `GB`, `GG`, `BG`, `BB`.\n",
    "\n",
    "\n",
    "1. Since we're conditioning on the older child being a girl, we eliminate `BG` and `BB`. Thus the only possibilities are `GB` and `GG`, equally likely. So the probability is $1/2$.\n",
    "2. Here, we're conditioning on the fact that one of them is a girl, so we only eliminate `BB`. Thus the remaining possibilities are `GB`, `GG`, `BG`, so the conditional probability of the other being a girl (i.e. 2 girls) is $1/3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conditional probabilities - formalizing things\n",
    "\n",
    "The key equation with conditional probabilities is\n",
    "\n",
    "$$P(A\\mid B)=\\frac{P(A \\cap B)}{P(B)}$$\n",
    "\n",
    "The \"renormalizing\" trick is a consequence of this. \n",
    "\n",
    "Consider, what's the probability of rolling a 6 given that the roll is not 1?\n",
    "\n",
    "- Let $A$ be the roll is a 6\n",
    "- Let $B$ be the roll is a not a 1\n",
    "\n",
    "$$P(A\\mid B)=\\frac{P(A \\cap B)}{P(B)}=\\frac{P(A)}{P(B)}=\\frac{1/6}{5/6}=\\frac{1}{5}$$\n",
    "\n",
    "In this case, we had the simplification that $P(A\\cap B)=P(A)$. This is often not the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayes' Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Brain teaser:** A heritable disease occurs randomly in 10% of the population. If someone has the disease, it is passed on to their children with probability 50%. A mother has 1 healthy child. Given this, what's the conditional probability that the mother has the disease? \n",
    "\n",
    "- Is the answer 10%? Less? More? How do we quantify it?\n",
    "  - Let $M$ be the event that the mother has the disease.\n",
    "  - Let $C$ be the event that the child has the disease.\n",
    "  - We want $P(M\\mid \\textrm{not } C)$. We have $P(M)=0.1$ and $P(\\textrm{not }C\\mid M)=0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Solution:\n",
    "\n",
    "$$P(M \\mid \\textrm{not } C) = \\frac{P(\\textrm{not } C \\mid M)P(M)}{P(\\textrm{not } C)}$$\n",
    "\n",
    "So we still need $P(\\textrm{not } C)$. This could happen in 2 ways (\"law of total probability\")\n",
    "\n",
    "$$P(\\textrm{not } C)=P(\\textrm{not } C \\mid M)P(M) + P(\\textrm{not } C \\mid \\textrm{not } M)P(\\textrm{not } M)$$\n",
    "\n",
    "We know $P(\\textrm{not } M)=1-P(M)=0.9$.   \n",
    "We assume $P( C \\mid \\textrm{not } M)=0.1$ because the child can randomly get the disease like anyone else,   \n",
    "so then $P(\\textrm{not } C \\mid \\textrm{not } M)=1-P( C \\mid \\textrm{not } M)=0.9$. \n",
    "\n",
    "Finally, then, we're left with:\n",
    "\n",
    "$$P(M \\mid \\textrm{not } C) = \\frac{0.5 \\times 0.1}{0.5\\times 0.1 + 0.9 \\times 0.9} = 0.058$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We can get what we need using **Bayes' Theorem**.\n",
    "- We've seen above that, for events $A$ and $B$, $P(A,B)=P(A\\mid B)P(B)$. \n",
    "- We can also write this as $P(A,B)=P(B\\mid A)P(A)$. \n",
    "- Since these are equal, we get the famous Bayes' theorem:\n",
    "​\n",
    "$$P(A\\mid B)=\\frac{P(B\\mid A)P(A)}{P(B)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If curious, you should also review:\n",
    "- Law of Total Probability  $P(X=x)=\\sum_y P(X=x\\mid Y=y)P(Y=y)$\n",
    "- Conditional Expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. \n",
    "\n",
    "Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n",
    "\n",
    "For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, all of these properties independently contribute to the probability that this fruit is an apple and that is why it is known as ‘Naive’.\n",
    "\n",
    "Naive Bayes model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods and works well with text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Multinomial Naive Bayes**\n",
    "\n",
    "You are working for Netflix. And you receive `positive` reviews for movies. You also receive `negative` reviews.\n",
    "\n",
    "You want to filter the `negative` reviews out so that you can move those movies out of the platform (and keep people binged).\n",
    "\n",
    "What would you do?\n",
    "\n",
    "Do you remember that for Logistic Regression we used CountVectorizer for SkLearn? This is because CountVectorizer\n",
    "counts all the words that are in positive (and negative) reviews.\n",
    "\n",
    "In Naive Bayes, we will calculate the probability of seeing each word GIVEN that it is in a `Positive` review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's say you have the phrase: \"Awesome movie\" and let's just focus on a total of 5 positve reviews and 5 negative reviews.\n",
    "\n",
    "The total words in the 5 positive reviews are 17.  The total words in negative reviews are 18.\n",
    "\n",
    "We calculate probabilities for each word. \n",
    "\n",
    "(I am not writing the exact reviews, so use your imagination for that)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example,\n",
    "assume the words in the positive reviews are:\n",
    "\n",
    "| Positive Reviews | Count | P(word,Pos) |\n",
    "| ----------- | -------- | ---------|\n",
    "|awesome\t|4| 0.22 |\n",
    "|movie\t|2| 0.11|\n",
    "|popcorn|\t3| 0.17|\n",
    "|exciting|\t4| 0.24|\n",
    "|terrific|\t4| 0.24|\n",
    "|trash|\t0| 0|\n",
    "|film |0 |0 |\n",
    "\n",
    "We guess a PRIOR probability that the message was originally a positive review. This is often assumed from the training set. Since we are doing 50% good reviews and 50% bad reviews, we can say that the prior probability for possitive reviews is 50%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We do the similar process for Negative Reviews\n",
    "\n",
    "| Negative Reviews | Count | P(word,Neg) |\n",
    "| ----------- | -------- | ---------|\n",
    "|awesome\t|1| 0.06 |\n",
    "|movie\t|4| 0.22|\n",
    "|popcorn|\t3| 0.16|\n",
    "|exciting|\t1| 0.06|\n",
    "|terrific|\t1| 0.06|\n",
    "|trash|\t7| 0.38|\n",
    "|film | 1 | 0.06 |\n",
    "\n",
    "Prior for Negative reviews is also 50%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We get a new review:\n",
    "\n",
    "\"film was awesome!\"\n",
    "\n",
    "How would you classify this review?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How does NB classify it?\n",
    "\n",
    "What do we need?   \n",
    "> $P(PPos) = 0.5$  \n",
    "> $P(awesome|pos) = 0.22$  \n",
    "> $P(film|pos) = 0.0$  \n",
    "\n",
    "$P(pos) = 0$\n",
    "\n",
    "Using the same algorithm, we also get a negative score:\n",
    "\n",
    "$P(neg) = 0.0018$  \n",
    "\n",
    "Review is classified as **NEGATIVE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How do we fix this?\n",
    "\n",
    "**Laplace Smoothing**\n",
    "\n",
    "In order to avoid having probabilities that alter our model,we add some extra counts (1 suffies most times) to each word. \n",
    "\n",
    "This parameter is called $\\alpha$ in sklearn.\n",
    "\n",
    "This *does not change* the prior probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's try again with $\\alpha = 1$\n",
    "\n",
    "| Positive Reviews | Count | P(word,Pos) |\n",
    "| ----------- | -------- | ---------|\n",
    "|awesome\t|3| 0.13 |\n",
    "|movie\t|5| 0.21|\n",
    "|popcorn|\t4| 0.17|\n",
    "|exciting|\t5| 0.21|\n",
    "|terrific|\t5| 0.21|\n",
    "|trash|\t1| 0.04|\n",
    "|film |1 |0.04 |\n",
    "\n",
    "\n",
    "$P(pos) = 0.0044$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "| Negative Reviews | Count | P(word, Neg) |\n",
    "| ----------- | -------- | ---------|\n",
    "|awesome\t|2| 0.08 |\n",
    "|movie\t|5| 0.2 |\n",
    "|popcorn|\t4| 0.16|\n",
    "|exciting|\t2| 0.08|\n",
    "|terrific|\t2| 0.08|\n",
    "|trash|\t8| 0.32|\n",
    "|film | 2 | 0.08 |\n",
    "\n",
    "$P(neg) = 0.0032$\n",
    "\n",
    "We can now classify the review as **positive**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Downsides** NB treats the words order the same ignoring all grammar rules and colloquial expressions. \n",
    "\n",
    "\\***** in a review, can mean 5 stars or it can be the reviewer using big words because the movie was boring.\n",
    "\n",
    "Another limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gaussian Naive Bayes**\n",
    "\n",
    "It can be used to predict over parameters where we have knowledge about the features distribution - Gaussian. Continuous data. \n",
    "\n",
    "We want to predict if someone would likes sushi or not. \n",
    "\n",
    "We collect Data from people that claim to like sushi and who claim to not like sushi.\n",
    "\n",
    "We collect data of how much fish and rice they eat each day.\n",
    "\n",
    "For people who like sushi:  \n",
    "Mean(grams of eaten fish daily) = 120, SD = 20  \n",
    "Mean(grams of eaten rice) = 100, SD = 40  \n",
    "Mean(grams of eaten beef) = 30, SD = 5  \n",
    "\n",
    "For people who don't like sushi:  \n",
    "Mean(grams of eaten fish daily) = 60, SD = 10  \n",
    "Mean(grams of eaten rice) = 90, SD = 40  \n",
    "Mean(grams of eaten beef) = 120, SD = 30  \n",
    "\n",
    "We assume that the data is represented by Gaussian distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new individual comes and they report eating:\n",
    "61 g of fish, \n",
    "100 g of rice, \n",
    "120 g of beef every day.\n",
    "\n",
    "Does this person like Sushi?\n",
    "\n",
    "We make an initial guess that they like Sushi.  \n",
    "Common guess comes from the training data.   \n",
    "\n",
    "And we get the $log(P(Prior_{likes-sushi})*L(fish | sushi)*L(rice | sushi) * L(beef | sushi))$\n",
    "\n",
    "We compute the same for does not like sushi.\n",
    "\n",
    "We compare the scores and make a decision based on that.\n",
    "\n",
    "We use log to avoid **OVERFLOW**\n",
    "Intial guess for not love A, is 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
